# OpenCompass 大模型评测
## 评测原因
* 随着大模型的蓬勃发展，如何全面系统地评估大模型的各项能力成为了亟待解决的问题。由于大语言模型和多模态模型的能力强大，应用场景广泛，目前学术界和工业界的评测方案往往只关注模型的部分能力维度，缺少系统化的能力维度框架与评测方案。
![image](https://github.com/baijiesong/InternLM_Learning/assets/105435837/ba50f768-4a66-4c45-bd4a-0f270ff81625)
## 评测方法
* 为了准确和公正地评估大模型的能力，国内外机构在大模型评测上开展了大量的尝试和探索。斯坦福大学提出了较为系统的评测框架HELM，从准确性，安全性，鲁棒性和公平性等维度开展模型评测。纽约大学联合谷歌和Meta提出了SuperGLUE评测集，从推理能力，常识理解，问答能力等方面入手，构建了包括8个子任务的大语言模型评测数据集。加州大学伯克利分校提出了MMLU测试集，构建了涵盖高中和大学的多项考试，来评估模型的知识能力和推理能力。谷歌也提出了包含数理科学，编程代码，阅读理解，逻辑推理等子任务的评测集Big-Bench，涵盖200多个子任务，对模型能力进行系统化的评估。
* 在中文评测方面，国内的学术机构也提出了如CLUE,CUGE等评测数据集，从文本分类，阅读理解，逻辑推理等方面评测语言模型的中文能力。
![image](https://github.com/baijiesong/InternLM_Learning/assets/105435837/b44b43cc-2df0-4bcc-8df4-f8de5a53ff90)
## opencompass
![image](https://github.com/baijiesong/InternLM_Learning/assets/105435837/ec789806-20fe-46cf-b7e7-0a020ee3d102)
### 评测对象
* 基座模型：一般是经过海量的文本数据以自监督学习的方式进行训练获得的模型（如OpenAI的GPT-3，Meta的LLaMA），往往具有强大的文字续写能力。
* 对话模型：一般是在的基座模型的基础上，经过指令微调或人类偏好对齐获得的模型（如OpenAI的ChatGPT、上海人工智能实验室的书生·浦语），能理解人类指令，具有较强的对话能力。
### 评测架构
![image](https://github.com/baijiesong/InternLM_Learning/assets/105435837/2dc7527f-287a-48d1-9161-0c66d6ef3595)
### 主观客观评测
* 客观：使用定量指标比较模型的输出与标准答案的差异，并根据结果衡量模型的性能。
* 主观：以人的主观感受为主的评测更能体现模型的真实能力，并更符合大模型的实际使用场景。
## 参考文档：https://github.com/InternLM/tutorial/blob/main/opencompass/opencompass_tutorial.md
## 参考视频：https://www.bilibili.com/video/BV1Gg4y1U7uc/
